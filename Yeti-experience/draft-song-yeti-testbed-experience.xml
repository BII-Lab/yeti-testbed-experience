<?xml version="1.0"?>

<!-- This template is for creating an Internet Draft using xml2rfc,
     which is available here: http://xml.resource.org. -->

<!DOCTYPE rfc SYSTEM "rfc2629.dtd" [
  <!-- One method to get references from the online citation libraries.
       There has to be one entity for each item to be referenced.
       An alternate method (rfc include) is described in the references. -->

  <!ENTITY RFC1034 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1034.xml">
  <!ENTITY RFC1035 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1035.xml">
  <!ENTITY RFC1996 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.1996.xml">
  <!ENTITY RFC2826 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.2826.xml">
  <!ENTITY RFC4968 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.4986.xml">
  <!ENTITY RFC5011 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5011.xml">
  <!ENTITY RFC5890 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.5890.xml">
  <!ENTITY RFC6891 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.6891.xml">
  <!ENTITY RFC7626 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7626.xml">
  <!ENTITY RFC7720 SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml/reference.RFC.7720.xml">

  <!ENTITY I-D.muks-dns-message-fragments SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-muks-dns-message-fragments-00.xml">
  <!ENTITY I-D.wkumari-dnsop-trust-management SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wkumari-dnsop-trust-management-01.xml">
  <!ENTITY I-D.wessels-edns-key-tag SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-wessels-edns-key-tag-00.xml">
  <!ENTITY I-D.andrews-tcp-and-ipv6-use-minmtu
    SYSTEM "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-andrews-tcp-and-ipv6-use-minmtu-04.xml">
  <!ENTITY I-D.bortzmeyer-dname-root SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-bortzmeyer-dname-root-00.xml">
  <!ENTITY I-D.ietf-dnsop-resolver-priming SYSTEM
    "http://xml2rfc.ietf.org/public/rfc/bibxml3/reference.I-D.draft-ietf-dnsop-resolver-priming-07.xml">
]>

<?xml-stylesheet type='text/xsl' href='rfc2629.xslt' ?>
<!-- used by XSLT processors -->
<!-- For a complete list and description of processing instructions (PIs),
     please see http://xml.resource.org/authoring/README.html. -->

<!-- Below are generally applicable Processing Instructions (PIs) that most
     I-Ds might want to use. (Here they are set differently than their
     defaults in xml2rfc v1.32) -->

<!-- give errors regarding ID-nits and DTD validation -->
<?rfc strict="yes" ?>

<!-- generate a ToC -->
<?rfc toc="yes"?>

<!-- control the table of contents (ToC) -->
<?rfc tocappendix="yes"?>

<!-- the number of levels of subsections in ToC. default: 3 -->
<?rfc tocdepth="3"?>

<!-- use symbolic references tags, i.e, [RFC2119] instead of [1] -->
<?rfc symrefs="yes"?>

<!-- sort the reference entries alphabetically -->
<?rfc sortrefs="yes" ?>

<!-- control vertical white space
     (using these PIs as follows is recommended by the RFC Editor) -->
<!-- do not start each main section on a new page -->
<?rfc compact="yes" ?>

<!-- keep one blank line between list items -->
<?rfc subcompact="no" ?>

<!-- end of list of popular I-D processing instructions -->

<?rfc comments="no" ?>
<?rfc inline="yes" ?>

<rfc category="info" docName="draft-song-yeti-testbed-experience-04"
    ipr="trust200902">

  <front>
    <title>Yeti DNS Testbed</title>

    <author fullname="Linjian Song" initials="L." surname="Song" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>songlinjian@gmail.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Dong Liu" initials="D." surname="Liu" role="editor">
      <organization>Beijing Internet Institute</organization>
      <address>
        <postal>
          <street>2508 Room, 25th Floor, Tower A, Time Fortune</street>
          <city>Beijing</city>
          <region></region>
          <code>100028</code>
          <country>P. R. China</country>
        </postal>
        <email>dliu@biigroup.com</email>
        <uri>http://www.biigroup.com/</uri>
      </address>
    </author>

    <author fullname="Paul Vixie" initials="P." surname="Vixie" role="editor">
      <organization>TISF</organization>
      <address>
        <postal>
        <street>11400 La Honda Road</street>
        <city>Woodside</city>
        <region>California</region>
        <code>94062</code>
        <country>US</country>
      </postal>
      <email>vixie@tisf.net</email>
      <uri>http://www.redbarn.org/</uri>
      </address>
    </author>

    <author fullname="Akira Kato" initials="" surname="Kato" role="editor">
      <organization>Keio University/WIDE Project</organization>
      <address>
        <postal>
          <street>Graduate School of Media Design, 4-1-1 Hiyoshi, Kohoku</street>
          <city>Yokohama</city>
          <region></region>
          <code>223-8526</code>
          <country>JAPAN</country>
        </postal>
        <email>kato@wide.ad.jp</email>
        <uri>http://www.kmd.keio.ac.jp/</uri>
      </address>
    </author>

    <date/>

    <!-- Meta-data Declarations -->

    <area>Internet Area</area>
    <workgroup>Internet Engineering Task Force (IETF)</workgroup>

    <!-- <keyword>dns</keyword> -->

    <abstract>
      <t>The Internet's Domain Name System (DNS) is built upon the
	foundation provided by the Root Server System -- that is,
	the critical infrastructure that serves the DNS root zone.</t>

      <t>Yeti DNS is an experimental, non-production testbed that
        aims to provide an environment where technical and operational
        experiments can safely be performed without risk to production
        infrastructure.  This testbed has been used by a broad
        community of participants to perform experiments that aim
        to inform operations and future development of the production
        DNS. Yeti DNS is an independently-coordinated project and
        is not affiliated with ICANN, IANA or any Root Server
        Operator.</t>

      <t>The Yeti DNS testbed implementation includes various novel
	and experimental components including IPv6-only transport,
	independent, autonomous Zone Signing Key management, large
	cryptographic keys and a large number of component Yeti-Root
	Servers. These differences from the Root Server System have
	operational consequences such as large responses to priming
	queries and the coordination of a large pool of independent
	operators; by deploying such a system globally but outside
	the production DNS system, the Yeti DNS project provides
	an opportunity to gain insight into those consequences
	without threatening the stability of the DNS.</t>

      <t>This document neither addresses the relevant policies under
	which the Root Server System is operated nor makes any
	proposal for changing any aspect of its implementation or
	operation. This document aims solely to document technical
        and operational findings following the deployment of a
        system which is similar but different from the Root Server
        System.</t>
    </abstract>
  </front>

  <middle>
    <section title="Introduction">
      <t>The Domain Name System (DNS), as originally specified in
        <xref target="RFC1034"/> and <xref target="RFC1035"/>, has
        proved to be an enduring and important platform upon which
        almost every user of Internet services relies. Despite its
        longevity, extensions to the protocol, new implementations
        and refinements to DNS operations continue to emerge both
        inside and outside the IETF.</t>

      <t>The Root Server System in particular has seen technical
        innovation and development in recent years, for example in
        the form of wide-scale anycast deployment, the mitigation
        of unwanted traffic on a global scale, the widespread
        deployment of Response Rate Limiting <xref target="RRL"/>,
        the introduction of IPv6 transport, the deployment of DNSSEC,
        changes in DNSSEC key sizes and preparations to roll the
        root zone's trust anchor. Together, even the projects listed
        in this brief summary imply tremendous operational change,
        all the more impressive when considered the necessary caution
        when managing Internet critical infrastructure, and the
        context of the adjacent administrative changes involved in
        root zone management and the (relatively speaking) massive
        increase in the the number of delegations in the root zone
        itself.</t>

      <t>Aspects of the operational structure of the Root Server
        System have been described in such documents as <xref
        target="TNO2009"/>, <xref target="ISC-TN-2003-1"/>, <xref
        target="RSSAC001"/> and <xref target="RFC7720"/>. Such
        references, considered together, provide sufficient insight
        into the operations of the system as a whole that it is
        straightforward to imagine structural changes to the root
        server system's infrastructure, and to wonder what the
        operational implications of such changes might be.</t>

      <t>The Yeti DNS Project was conceived in May 2015 to provide
        a captive, non-production testbed upon which the technical
        community could propose and run experiments designed to
        answer these kinds of questions. Coordination for the project
        was provided by TISF, the WIDE Project and the
        Beijing Internet Institute. Many volunteers collaborated
        to build a distributed testbed that at the time of writing
        includes 25 Yeti root servers with 16 operators and
        handles experimental traffic from individual volunteers,
        universities, DNS vendors and distributed measurement
        networks.</t>

      <t>By design, the Yeti testbed system serves the root zone
	published by the IANA with only those structural modifications
	necessary to ensure that it is able to function usefully
	in the Yeti testbed system instead of the production Root
	Server system.  In particular, no delegation for any top-level
	zone is changed, added or removed from the IANA-published
	root zone to construct the root zone served by the Yeti
	testbed system. In this document, for clarity, we refer to
	the zone derived from the IANA-published root zone as the
	Yeti-Root zone.</t>

      <t>The Yeti DNS testbed serves a similar function to the Root
	Server System in the sense that they both serve similar
	zones (the Yeti-Root zone and the Root zone, respectively).
	However, the Yeti DNS testbed only serves clients that are
	explicitly configured to participate in the experiment,
	whereas the Root Server System serves the whole Internet.
	The known set of clients has allowed structural changes to
	be deployed in the Yeti DNS testbed whose impact on clients
	can be measured and analysed.</t>
    </section>

    <section title="Controversy">
      <t>The Yeti DNS Project, its infrastructure and the various
	experiments that have been carried out using that infrastructure,
	have been described by people involved in the project in
	many public meetings at technical venues since its inception.
	The mailing lists using which the operation of the
	infrastructure has been coordinated are open to join, and
	their archives are public. The project as a whole has been
	the subject of robust public discussion.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
	Project is, in effect, operating an "alternate root,"
	challenging the IAB's comments published in <xref
	target="RFC2826"/>. Other such alternate roots are considered
	to have caused end-user confusion and instability in the
	namespace of the DNS by the introduction of new top-level
	labels or the different use of top-level labels present in
	the Root Server System. The coordinators of the Yeti DNS
	Project do not consider the Yeti DNS Project to be an
	alternate root in this sense, since by design the namespace
	enabled by the Yeti-Root Zone is identical to that of the
	Root Zone.</t>

      <t>Some commentators have expressed concern that the Yeti DNS
	Project seeks to influence or subvert administrative policy
	relating to the Root Server System, in particular in the
	use of DNSSEC trust anchors not published by the IANA and
	the use of Yeti-Root Servers in regions where governments
	or other organisations have expressed interest in operating
	a Root Server. The coordinators of the Yeti-Root project
	observe that their mandate is entirely technical and has
	no ambition to influence policy directly; they do hope,
	however, that technical findings from the Yeti DNS Project
	might act as a useful resource for the wider technical
	community.</t>

      <t>Finally, some concern has been expressed about the possible
	applications of the Yeti DNS Project to the governments of
	countries where access to the Internet is subject to
	substantial centralised control, in contrast to most other
	jurisdictions where such controls are either lighter or not
	present. The coordinators of the Yeti DNS Project have taken
	care to steer all discussions and related decisions about
	the technical work of the project to public venues in the
	interests of full transparency, and encourage anybody
	concerned about the decision-making process to participate
	in those venues and review their archives directly.</t>
    </section>

    <section title="Areas of Study">
     <t>Examples of topics that the Yeti DNS Testbed was built to
       address are included below, each illustrated with indicative
       questions.</t>

     <section title="Implementation of a Root Server System-like Testbed">
       <t>
         <list style="symbols">
           <t>How can a captive testbed be constructed and deployed
             on the Internet, allowing useful public participation
             without any risk of disruption of the Root Server System?</t>

	   <t>How can representative traffic be introduced into
	     such a captive testbed such that insights into the
	     impact of specific differences between the testbed and
	     the Root Server System can be observed?</t>
         </list>
       </t>
     </section>

      <section title="Yeti-Root Zone Distribution">
        <t>
          <list style="symbols">
            <t>What are the scaling properties of Yeti-Root zone
              distribution as the number of Yeti-Root servers,
              Yeti-Root server instances or intermediate distribution
              points increase?</t>
          </list>
        </t>
      </section>

      <section title="Yeti-Root Server Names and Addressing">
        <t>
          <list style="symbols">
            <t>What naming schemes other than those closely analogous
              to the use of ROOT-SERVERS.NET in the production root
              zone are practical, and what are their respective
              advantages and disadvantages?</t>

            <t>What are the risks and benefits of signing the zone that
              contains the names of the Yeti-Root servers?</t>

            <t>What automatic mechanisms might be useful to improve
              the rate at which clients of Yeti-Root servers are
              able to react to a Yeti-Root server renumbering
              event?</t>
          </list>
        </t>
      </section>

      <section title="IPv6-Only Yeti-Root Servers">
        <t>
          <list style="symbols">
            <t>Are there negative operational effects in the use of
              IPv6-only Yeti-Root servers, compared to the use of
              servers that are dual-stack?</t>

            <t>What effect does the IPv6 fragmentation model have on the
              operation of Yeti-Root servers, compared with that of IPv4?</t>
          </list>
        </t>
      </section>

      <section title="DNSSEC in the Yeti-Root Zone">
        <t>
          <list style="symbols">
            <t>Is it practical to sign the Yeti-Root zone using multiple,
              independently-operated DNSSEC signers and multiple
              corresponding ZSKs?</t>

            <t>To what extent is <xref target="RFC5011"/> supported by
              resolvers?</t>

            <t>Does the KSK Rollover plan designed and in the process
              of being implemented by ICANN work as expected on the
              Yeti testbed?</t>

            <t>What is the operational impact of using much larger RSA
              key sizes in the ZSKs used in the Yeti-Root?</t>

            <t>What are the operational consequences of choosing
              DNSSEC algorithms other than RSA to sign the Yeti-Root
              zone?</t>
          </list>
        </t>
      </section>
    </section>

    <section title="Yeti DNS Testbed Infrastructure">
      <t>The purpose of the testbed is to allow DNS queries from
	stub resolvers, mediated by recursive resolvers, to be
	delivered to Yeti-Root servers, and for corresponding
	responses generated on the Yeti-Root servers to be returned,
	as illustrated in <xref target="components"/>.</t>

      <figure anchor="components" title="High-Level Testbed Components">
        <artwork>
          <![CDATA[
    ,----------.        ,-----------.        ,------------.
    |   stub   +------> | recursive +------> | Yeti-Root  |
    | resolver | <------+ resolver  | <------+ nameserver |
    `----------'        `-----------'        `------------'
       ^                   ^                    ^
       |  appropriate      |  Yeti-Root hints;  |  Yeti-Root zone
       `- resolver         `- Yeti-Root trust   `- with DNSKEY RRSet,
          configured          anchor               signed by Yeti-KSK
]]>
        </artwork>
      </figure>

      <t>To use the Yeti DNS testbed, a stub resolver must be
        explicitly configured to use recursive resolvers that
        have themselves been configured to use the Yeti-Root
        servers.  On the resolvers, that configuration consists
        of a list of names and addresses for the Yeti-Root servers
        (often referred to as a "hints file") that replaces the
        normal Internet DNS hints. Resolvers also need to be
        configured with a DNSSEC trust anchor that corresponds
        to a KSK used in the Yeti DNS Project, in place of the
        normal trust anchor for the root zone.</t>

      <t>The need for a Yeti-specific trust anchor in the resolver
	stems from the need to make minimal changes to the root
	zone, as retrieved from the IANA, to transform it into the
	Yeti-Root that can be used in the testbed.  Those changes
	would be properly rejected by any validator using an accurate
	root zone trust anchor as bogus. Corresponding changes are
	required in the Yeti-Root hints file
        <xref target="yeti-hints"/>.</t>

      <t>The data flow from IANA to stub resolvers through the
        Yeti testbed is illustrated in <xref target="yeti-root"/>
        and are described in more detail in the sections that
        follow.</t>

      <figure anchor="yeti-root" title="Testbed Data Flow">
        <artwork>
          <![CDATA[
                               ,----------------.
                          ,-- / IANA Root Zone / ---.
                          |  `----------------'     |
                          |            |            |
                          |            |            |       Root Zone
  ,--------------.    ,---V---.    ,---V---.    ,---V---.
  | Yeti Traffic |    | BII   |    | WIDE  |    | TISF  |
  | Collection   |    |  DM   |    |  DM   |    |  DM   |
  `----+----+----'    `---+---'    `---+---'    `---+---'
       |    |       ,-----'    ,-------'            `----.
       |    |       |          |                         |
       ^    ^       |          |                         |  Yeti-Root
       |    |   ,---V---.  ,---V---.                 ,---V---.
       |    `---+ Yeti  |  | Yeti  |  . . . . . . .  | Yeti  |
       |        | Root  |  | Root  |                 | Root  |
       |        `---+---'  `---+---'                 `---+---'
       |            |          |                         |    DNS
       |            |          |                         |  Response
       |         ,--V----------V-------------------------V--.
       `---------+              Yeti Resolvers              |
                 `--------------------+---------------------'
                                      |                       DNS
                                      |                     Response
                 ,--------------------V---------------------.
                 |            Yeti Stub Resolvers           |
                 `------------------------------------------'
]]>
        </artwork>
      </figure>

      <section title="Root Zone Retrieval">
        <t>Since Yeti DNS servers cannot receive DNS NOTIFY
          <xref target="RFC1996"/> messages from the Root Server
	  System, a polling approach is used. Each Yeti Distribution
	  Master (DM) requests the root zone SOA record from a
	  nameserver that permits unauthenticated zone transfers
	  of the root zone, and performs a zone transfer from that
	  server if the retrieved value of SOA.SERIAL is greater
	  than that of the last retrieved zone.</t>

	 <t>At the time of writing, unauthenticated zone transfers
	  of the root zone are available directly from B-Root,
	  C-Root, F-Root, G-Root and K-Root, and from L-Root via
	  the two servers XFR.CJR.DNS.ICANN.ORG and XFR.LAX.DNS.ICANN.ORG,
	  as well as via FTP from sites maintained by the Root Zone
	  Maintainer and the IANA Functions Operator. The Yeti DNS
	  Testbed retrieves the root zone from using zone transfers
	  from F-Root. The schedule on which F-Root is polled by
	  each Yeti DM is as follows:</t>

        <texttable>
          <ttcol>DM Operator</ttcol><ttcol>Time</ttcol>
          <c>BII</c><c>UTC hour + 00 minutes</c>
          <c>WIDE</c><c>UTC hour + 20 minutes</c>
          <c>TISF</c><c>UTC hour + 40 minutes</c>
        </texttable>

        <t>The Yeti DNS testbed uses multiple DMs, each of which acts
	  autonomously and equivalently to its siblings. Any single
	  DM can act to distribute new revisions of the Yeti-Root
	  zone, and is also responsible for signing the RRSets that
	  are changed as part of the transformation of the Root
	  Zone into the Yeti-Root zone described in <xref
	  target="transformation"/>. This shared control over the
	  processing and distribution of the Yeti-Root zone
	  approximates some of the ideas around shared zone control
	  explored in <xref target="ITI2014"/>.</t>
      </section>

      <section title="Transformation of Root Zone to Yeti-Root Zone"
        anchor="transformation">

        <t>Davey's note: The routine of DM are documented in : https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Setup.md. Regarding the SOA record, it is said "The MNAME and RNAME are set to Yeti values". Currently we implement like this: MNAME: www.yeti-dns.org. RNAME: DM-operator.yeti-dns.org. Such as wide.yeti-dns.org for WIDE and bii.yeti-dns.org for BII.</t>

        <t>XXX(need description of the precise process followed here)XXX</t>

        <t>XXX(remove RRSIG)XXX</t>

        <t>XXX(remove apex NS RRSet)XXX</t>

        <t>XXX(add apex Yeti NS RRSet)XXX</t>

        <t>XXX(remove apex DNSKEY RRSet)XXX</t>

        <t>XXX(add apex Yeti DNSKEY RRSet)XXX</t>

        <t>XXX(sign zone using specified keys)XXX</t>

        <t>The transformation of Root Zone to Yeti-Root zone takes
          place independently on each of the Yeti DMs.</t>

        <t>Note SOA.SERIAL preservation during transformation process.</t>
      </section>

      <section title="Yeti-Root Zone Distribution">
        <t>Davey's note: I re-sent mail with title : DM configuration</t>

        <t>XXX(need details about NOTIFY configuration on Yeti-DMs)XXX</t>

        <t>XXX(need details about TSIG authentication on Yeti-DMs)XXX</t>

        <t>XXX(examples of configuration from DMs and Yeti-Root servers
          might be handy)XXX</t>
      </section>

      <section title="Information Synchronization">
        <t>XXX(Joe needs more information about the content of this section
          before he can suggest changes)XXX</t>

        <t>Given three DMs operational in Yeti root system, it is
          necessary to prevent any inconsistency caused by human
          mistakes in operation.  The straight method is to share
          the same parameters to produce the Yeti root zone.  There
          parameters includes following set of files:</t>

        <t>
          <list style="symbols">
            <t>the list of Yeti root servers, including:
              <list style="symbols">
                <t>public IPv6 address and host name </t>
                <t>IPv6 addresses originating zone transfer</t>
                <t>IPv6 addresses to send DNS notify to</t>
              </list>
            </t>
            <t>the ZSKs used to sign the root</t>
            <t>the KSK used to sign the root</t>
            <t>the SERIAL when this information is active</t>
          </list>
        </t>

        <t>XXX(that list describes the contents of files, not the files
          themselves. Can I see examples of the actual files that are
          used?)XXX</t>

	<t>Davey's Note: I sent you a mailing with the title:
	  directories and files in DM's repo.</t>

        <t>The operation is simple that each DM operator synchronize
          the files with the information needed to produce the Yeti
          root zone. XXX(how?)XXX  When a change is desired XX(by
          whom?)XX (such as adding a new server or rolling the ZSK),
          a DM operator updates the local file and push to other
          DM XX(how?)XX.  A SOA SERIAL in the future is chosen for
          when the changes become active. </t>
      </section>

      <section title="Yeti-Root Server Naming Scheme">
	<t>The current naming scheme for Root Servers was normalized
	  to use single-character host names (A through M) under
	  the domain ROOT-SERVERS.NET, as described in <xref
	  target="RSSAC023"/>). The principal benefit of this naming
	  scheme is that DNS label compression can be used to produce
	  a priming response that will fit within 512 bytes, the
	  maximum DNS message size using UDP transport without EDNS0
	  <xref target="RFC6891"/>.</t>

        <t>Yeti-Root Servers do not use this optimisation, but rather
          use free-form nameserver names chosen by their respective
	  operators -- in other words, no attempt is made to minimise
	  the size of the priming response through the use of label
	  compression. This approach aims to challenge the need for
	  a minimally-sized priming response in a modern DNS ecosystem
	  where EDNS(0) is prevalent.</t>

        <t>Priming responses from Yeti-Root Servers do not always
	  include server addresses in the additional section, as
	  is the case with priming responses from Root Servers. In
	  particular, Yeti-Root Servers running BIND9 return an
	  empty additional section, forcing resolvers to complete
	  the priming process with a set of conventional recursive
	  lookups in order to resolve addresses for each Yeti-Root
	  server. Yeti-Root Servers running NSD appeared to return
          a fully-populated additional section.</t>

        <t>Various approaches to normalise the composition of the
          priming response were considered, including:

          <list style="symbols">
            <t>Require use of DNS implementations that exhibit
              the desired behaviour in the priming response (e.g.
              NSD) in favour of BIND9;</t>

            <t>Modification of BIND9 (and any other server with
              similar behaviour) for use by Yeti-Root Servers;</t>

	    <t>Isolate the names of Yeti-Root Servers in one or
	      more zones that could be slaved on each Yeti-Root
	      Server, renaming servers as necessary, giving each a
	      source of authoritative data with which the authority
	      section of a priming response could be fully populated.
	      This is the approach used in the Root Server System.</t>
	  </list>
	</t>

        <t>The potential mitigation of renaming all Yeti-Root Servers
          using a scheme that would allow their names to exist in the
          balliwick of the root zone was not considered, since that
          approach implies the invention of new top-level labels not
          present in the Root Zone.</t>

	<t>Given the relative infrequency of priming queries by
	  individual resolvers and the additional complexity or
	  other compromises implied by each of those mitigations,
	  the decision was made to make no effort to ensure that
	  the composition of priming responses was identical across
	  servers. Even the empty additional sections generated by
	  Yeti-Root Servers running BIND9 seem to be sufficient for
	  all resolver software tested; resolvers simply perform a
	  new recursive lookup for each authoritative server name
	  they need to resolve.</t>
      </section>

      <section title="Yeti Root Servers">
        <t>Various volunteers have donated authoritative servers
          to act as Yeti-Root servers. At the time of writing there
          are 25 Yeti-Root servers distributed globally, one of
          which is named using an IDNA2008 <xref target="RFC5890"/>
          label, shown in the following list in punycode.</t>

        <texttable>
          <ttcol>Name</ttcol><ttcol>Operator</ttcol><ttcol>Location</ttcol>

          <c>bii.dns-lab.net</c><c>BII</c><c>CHINA</c>

          <c>yet-ns.tsif.net</c><c>TSIF</c><c>USA</c>

          <c>yeti-ns.wide.ad.jp</c><c>WIDE Project</c><c>Japan</c>

          <c>yeti-ns.as59715.net</c><c>as59715</c><c>Italy</c>

          <c>dahu1.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

	  <c>ns-yeti.bondis.org</c><c>Bond Internet Systems</c><c>Spain</c>

          <c>yeti-ns.ix.ru</c><c>Russia</c><c>MSK-IX</c>

          <c>yeti.bofh.priv.at</c><c>CERT Austria</c><c>Austria</c>

          <c>yeti.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

	  <c>yeti-dns01.dnsworkshop.org</c><c>dnsworkshop
	    /informnis</c><c>Germany</c>

          <c>yeti-ns.conit.co</c><c>CONIT S.A.S </c><c>Colombia</c>

          <c>dahu2.yeti.eu.org</c><c>Dahu Group</c><c>France</c>

          <c>yeti.aquaray.com</c><c>Aqua Ray SAS</c><c>France</c>

          <c>yeti-ns.switch.ch</c><c>SWITCH</c><c>Switzerland</c>

          <c>yeti-ns.lab.nic.cl</c><c>CHILE NIC</c><c>Chile</c>

          <c>yeti-ns1.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns2.dns-lab.net</c><c>BII</c><c>China</c>

          <c>yeti-ns3.dns-lab.net</c><c>BII</c><c>China</c>

          <c>ca...a23dc.yeti-dns.net</c><c>Yeti-ZA</c><c>South Africa</c>

          <c>3f...374cd.yeti-dns.net</c><c>Yeti-AU</c><c>Australia</c>

          <c>yeti1.ipv6.ernet.in</c><c>ERNET India</c><c>India</c>

	  <c>xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c</c><c>ERNET
	    India</c><c>India</c>

	  <c>yeti-dns02.dnsworkshop.org</c><c>dnsworkshop
	    /informnis</c><c>USA</c>

	  <c>yeti.mind-dns.nl</c><c>Monshouwer Internet
	    Diensten</c><c>Netherlands</c>

          <c>yeti-ns.datev.net</c><c>DATEV</c><c>Germany</c>
        </texttable>

        <t>The current list of Yeti-Root server is made available
          to a participating resolver first using a substitute
          hints file <xref target="yeti-hints"/> and subsequently
          by the usual resolver priming process
          <xref target="I-D.ietf-dnsop-resolver-priming"/>. All
          Yeti-Root servers are IPv6-only, foreshadowing a future
          IPv6-only Internet, and hence the Yeti-Root hints file
          contains no IPv4 addresses and the Yeti-Root zone
          contains no IPv4 glue.</t>

        <t>At the time of writing, all root servers within the
	  Root Server System serve the ROOT-SERVERS.NET zone in
	  addition to the root zone, and all but one also serve the
	  ARPA zone. Yeti-Root servers serve the Yeti-Root zone
	  only.</t>

        <t>Significant software diversity exists across the set of
          Yeti-Root servers, as reported by their volunteer operators:

          <list style="symbols">
            <t>Platform: 20 of 25 Yeti-Root servers are implemented
              on a VPS rather than bare metal.</t>
	    <t>Operating System: 6 Yeti-Root servers run on
	      on Linux (Ubuntu, Debian, CentOS, and ArchLinux); 5
	      run on FreeBSD and 1 on NetBSD. The remainder were not
              disclosed (XXX really? surely we can find this out. Davey's Note I have update information. I will send to you later)</t>
	    <t>DNS software: 18 of 25 Yeti-Root servers use BIND9
	      (versions varying between 9.9.7 and 9.10.3); four use
	      NSD (4.10 and 4.15); two use Knot (2.0.1 and 2.1.0)
	      and one uses Bundy (1.2.0).</t>
	  </list>
	</t>
      </section>

      <section title="Traffic Capture and Analysis">
	<t>Query and response traffic capture is available in the
	  testbed in both Yeti resolvers and Yeti-Root servers in
	  anticipation of experiments that require packet-level
	  visibility into DNS traffic.</t>

        <t>Traffic capture is performed on Yeti-Root servers using
	  either dnscap <eref
	  target="https://www.dns-oarc.net/tools/dnscap"/> or
	  pcapdump (part of the pcaputils Debian package <eref
	  target="https://packages.debian.org/sid/pcaputils"/>,
	  with a patch to facilitate triggered file upload <eref
	  target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985"/>.
	  PCAP-format files containing packet captures are uploaded
	  using (XXX check mechanism) to a central repository.</t>
      </section>
    </section>

    <section title="Experience with Yeti DNS">
      <t>As introduced, Yeti DNS is a root testbed designed for 
        trying new ideas. For each proposed idea, it is firstly 
        tested in Lab environment as a proof of concept before it 
        is implemented in the testbed. The approach is straightforward that 
        we set the system up according to the experiment proposal
        with monitoring on the data flow and resolution path. 
        Operational issues, impacts and complexities are observed 
        and discussed mainly in the mailing list. Some experiments 
        and findings are summarized in the blog of Yeti DNS Project. 
        This section introduces the experience with Yeti DNS.</t>

      <t>Davey's note: we have done other experiments and document
	them in yeti blog.  If you find them useful we can add to
	experiment section:

        <list style="numbers">
          <t><eref target="http://yeti-dns.org/yeti/blog/2017/03/26/Monitoring-on-Yeti-Root-Zone-Update.html"/></t>

          <t><eref target="http://yeti-dns.org/yeti/blog/2017/03/03/bind-edns-fallback-and-dn.ssec-issue.html"/></t>

	  <t><eref target="http://yeti-dns.org/yeti/blog/2016/12/20/Scoring-the-Yeti-DNS-Root-Server-System.html"/></t>

          <t><eref target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html"/></t>
        </list>
      </t>

      <section title="Multiple-ZSK Operations">
	<t>Davey's note: There are two documents in repo which will
	  helpfull for you: <eref
	  target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Report-MZSK.md"/>
	  and <eref
	  target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Experiment-MZSK.md"/>.</t>

        <t>The Root Server System incorporates a root zone signed
	  using DNSSEC. Responsibility for cryptographic operations
	  is split between the IANA Functions Operator (PTI), who
	  takes responsibility for management of the Key Signing
	  Key (KSK) and associated trust anchor, and the Root Zone
	  Maintainer (VeriSign) who manages the Zone Signing Key
	  (ZSK).</t>

	<t>There is no known operational concern with this approach.
	  However, analogous to the established principle of using
	  multiple, independent operators to make the Root Server
	  System robust, an alternative system was implemented in
	  the Yeti DNS Testbed to add diversity to DNSSEC operations
	  and assess the impact of any additional complexity of
	  such an approach on the stability of the Yeti DNS Testbed
	  as a whole.</t>

	<t>The Multi-ZSK experiment is designed to test operating
	  the Yeti root using more than a single ZSK. The goal is
	  to have each distribution master (DM) have a separate
	  ZSK, signed by a single KSK. This will allow DM to operate
	  independently, each maintaining their own key secret
	  material.</t>

        <figure anchor="mulit-zsk" title="Multiple ZSK Experiment">
          <artwork>
            <![CDATA[
                       +--------------+         +-----------+
              +------->| BII ZSK      |-------->| Yeti root |
              | signs  +--------------+  signs  +-----------+
              |
+-----------+ |        +--------------+         +-----------+
| Yeti KSK  |-+------->| TISF ZSK     |-------->| Yeti root |
+-----------+ | signs  +--------------+  signs  +-----------+
              |
              |        +--------------+         +-----------+
              +------->| WIDE ZSK     |-------->| Yeti root |
                signs  +--------------+  signs  +-----------+
]]>
          </artwork>
        </figure>

	<t>In the MZSK setup, each DM maintains a separate ZSK and
	  produces a version of the root zone with different
	  signatures.  Aside from the signatures, the contents of
	  the Yeti root zone is identical, as shown in  <xref
	  target="mulit-zsk"/>.</t>

	<t>In DNSSEC, the KSK and ZSK are all returned in a single
	  value, the DNSKEY Resource Record Set (RRset). Because
	  of this, DNSSEC resolvers are able to validate answers
	  as long as they are signed by any of the ZSK.</t>

	<t>This experiment will test the behavior of the Yeti root
	  servers and resolvers as we have multiple ZSK, both during
	  normal operation and during rollover. We did not change
	  the algorithm used, the key lengths, or the timings.
	  ***Davey: algorithm, key lengths and timing is ought to
	  be introduce in section 3, architecture part****</t>

          <section title="Proof of Concept">
            <t>XXX There is some detail missing from this description of
	      the environment tested in the lab. Presuably "A and B"
	      are placeholders for Yeti-Root servers deployed in a
	      captive environment to test, and not related to actual
	      Yeti-Root servers. The phrase "resolver switch" is not
	      familiar, and needs explanation. There's no detail about
	      the algorithms or key sizes used in the ZSKs, how the
	      root zone apex DNSKEY RRSet was constructed, what
	      signatures by the KSK were involved, and what trust
	      anchors were configured in validators. The commentary
	      on response size is suggestive of experimentation, but
	      no experiment is described. We need more detail of what
	      happened here for its inclusion in this document to be
	      useful. XXX </t>

          <t>Davey's notes: a full report on this lab test:
	    <eref
	    target="http://lists.yeti-dns.org/pipermail/discuss/attachments/20151021/01fd3827/attachment.txt"/>.
	    Right now I just use a brief description below on this
	    lab test to show the result.</t>

	  <t>In preparation for the experiment, a lab test was done
	    to verify the behavior of DNS resolvers under controlled
	    conditions. A DNS resolver was configured using two
	    experimental root servers. Traffic was sent insuring
	    that the resolver had the ZSK from one or both of the
	    servers, and then the validation results checked. The
	    lab test confirmed that if a root server tried to use
	    a ZSK that was not present in the DNSKEY RRset that it
	    would fail, but that even if two servers used separate
	    ZSK this resolves properly as long as they are both
	    present in the DNSKEY RRset.</t>
	</section>

	<section title="Experiment">
	  <t>After the lab test, the MZSK experiment is being
	  conducted
	    on the Yeti platform. There are two phases:</t>

          <t>XXX It's not entirely obvious that Phase 1 here does not
            correspond to the lab experiment described above in "Proof
            of Concept". Is it different? Is there a more detailed
            description of what was done, here? XXX</t>

          <t>Davey's note:Phase 1 is different from proof of concept because it is done in the Yeti root testbed not lab test. You can refer to the link: https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Experiment-MZSK.md#phase-1 </t>

          <t>
            <list style="symbols">
	      <t>Phase 1. In the first phase, we confirmed that
		using multiple ZSKs works in the wild. We insured
		that using the maximum number of ZSKs continues to
		work in the resolver side.  Here one of the DM (BII)
		created and added 5 ZSKs using the existing
		synchronization mechanism.  (If all 3 ZSKs are
		rolling then we have 6 total. To get this number
		we add 5.)</t>

              <t>Phase 2. In the second phase, we delegated the
                management of the ZSKs so that each DM creates and
                publishes a separate ZSK. For this phase, modified
                zone generation protocol and software was used <xref
                target="Yeti-DM-Sync-MZSK"/>, which allows the DM to
                sign without access to the private parts of ZSKs
                generated by other DMs. In this phase we roll all
                three ZSKs separately.</t>
            </list>
          </t>
        </section>

        <section title="Conclusion">
	  <t>XXX There are not really any substantive results
	    described in the text below. Presumably the core
	    conclusion is that the approach with multiple ZSK
	    managers works, but to answer the question we want to
	    know about failure modes, operational complexity, etc
	    -- the notes below suggest success on the grounds of
	    "we got it running after some difficulties" but say
	    nothing about the other stuff. XXX</t>

      <t>Davey's note: The two documents about MZSK experiment may give you some clue. In general in my understand, we are not sure 1) DNSSEC resolvers continue to function properly with a MZSK setup (may fail?) 2)Yeti system would continue to function properly with an MZSK setup(do we need any specially setup ?) 3)the impact for large response (we report one error due to the failure)</t>

	  <t>The MZSK experiment was finished by the end of 2016-04.
	    Almost everything appears to be working. But there have
	    been some findings <xref target="Experiment-MZSK-notes"/>,
	    including discovering that IPv6 fragmented packets are
	    not forwarded on an Ethernet bridge with netfilter
	    ip6_tables loaded on one authority server, and issue
	    with IXFR falling back to AXFR due to multiple signers
	    which will be detailed in a separate draft describing
	    as a problem statement.</t>
	</section>
      </section>

      <section title="Root Renumbering Issue and Hint File Update">
	<t>XXX This experiment seems to be more about testing a
	  script than finding something out about the root server
	  system using a testbed. Presumably the tool described
	  here could be more conventionally tested using a set of
	  unit tests using (e.g.) vagrant. What is the experiment
	  here that requires the use of the Yeti testbed? The final
	  paragraph suggests that the testbed provides a useful
	  place for this work because in the Yeti-Root the root
	  zone apex NS set is signed, whereas in the root zone it
	  is not. But that is demonstrably incorrect; the root zone
	  apex NS set *is* signed. XXX</t>

    <t>Davey's note: I think we could put it into another section or subsection : Yeti tools. Right now we did not document them. Actually we have some tools developed for Yeti. Let's me give you some links: 1)http://yeti-dns.org/yeti/blog/2016/06/15/Mirroring-traffic-using-dnsdist.html 2) http://dnsv6lab.net/2016/09/06/DNS-pcap-fragments/ 3) http://dnsv6lab.net/2016/10/13/ymmv/ </t>

	<t>With the recent renumbering of H root Server's IP address,
	  there is a discussion of ways that resolvers can update
	  their hint file.  Traditional ways include using FTP
	  protocol by doing a wget and using dig to double-check
	  the servers' addresses manually.  Each way would depend
	  on manual operation.  As a result, there are many old
	  machines that have not updated their hint files.  As a
	  proof, after completion of renumbering in thirteen years
	  ago, there is an observation that the "Old J-Root" can
	  still receive DNS query traffic <xref
	  target="Renumbering-J-Root"/>.</t>

	<t>This experiment proposal aims to find an automatic way
	  for hint-file updating.  The already-completed work is a
	  shell script tool which provides the function that updates
	  a hint-file in file system automatically with DNSSEC and
	  trust anchor validation.  <xref
	  target="Hintfile-Auto-Update"/>.</t>

	<t>The methodology is straightforward.  The tool first
	  queries the NS list for "." domain and queries A and AAAA
	  records for every name on the NS list.  It requires DNSSEC
	  validation for both the NS list and the A and AAAA answers.
	  After getting all the answers, the tool compares the new
	  hint file with the old one.  If there is a difference,
	  it renames the old one with a time-stamp and replaces the
	  old one with the new one.  Otherwise the tool deletes the
	  new hint file and nothing will be changed.</t>

        <t>Note that in current IANA root system the servers named
          in the root NS record are not signed. So the tool can not
          fully work in the production network.  In Yeti root system
          some of the names listed in the NS record are signed,
          which provides a test environment for such a proposal.</t>
      </section>

      <section title="Fragmentation of DNS Responses">
        <t>XXX This section starts to identify a problem, but does not
          actually describe an experiment, any results or a conclusion.
          Was an experiment carried out? Where is it described? XXX</t>

          <t>Davey's note: Actually we just observe bugs or weird issues 
            during the IPv6-only root operation. One possible way is to 
            put it as an collection of fingds or report on issues we observed. 
            Right now I have a dataset from Atlas probes which give me some 
            hint that IPv6 fragmentation caused by large response do increase 
            the failure rate using UDP. I may introduce it later when we meet in Prague.</t>

        <t>In consideration of new DNS protocol and operation, there
          is always a hard limit on the DNS packet size.  Take Yeti
          for example: adding more root servers, using the Yeti
          naming scheme, rolling the KSK, and Multi-ZSK all increase
          the packet size.  The fear of large DNS packets mainly
          stem from two aspects: one is IP-fragments and the other
          is frequently falling back to TCP.</t>

        <t>Fragmentation may cause serious issues; if one of the
          fragment is lost at random, it results in the loss of
          entire packet and involve timeout.  If the fragment is
          dropped by a middle-box, the query always results in
          failure, and result in name resolution failure unless the
          resolver falls back to TCP. It is known at this moment
          that limited number of security middle-box implementations
          support IPv6 fragments.</t>

        <t>A possible solution is to split a single DNS message
          across multiple UDP datagrams.  This DNS fragments mechanism
          is documented in <xref target="I-D.muks-dns-message-fragments"/>
          as an experimental IETF draft.</t>
      </section>

      <section title="The KSK Rollover Experiment in Yeti">
        <t>The Yeti DNS Project provides a good basis to conduct a
          real-world experiment of a KSK rollover in the root zone.
          It is not a perfect analogy to the IANA root because all
          of the resolvers to the Yeti experiment are "opt-in", and
          are presumably run by administrators who are interested
          in the DNS and knowledgeable about it. Still, it can
          inform the IANA root KSK roll.</t>

        <t>The IANA root KSK has not been rolled as of the writing.
          ICANN put together a design team to analyze the problem
          and make recommendations.  The design team put together
          a plan<xref target="ICANN-ROOT-ROLL"/>.  The Yeti DNS
          Project may evaluate this scenario for an experimental
          KSK roll.  The experiment may not be identical, since the
          time-lines laid out in the current IANA plan are very
          long, and the Yeti DNS Project would like to conduct the
          experiment in a shorter time, which may considered much
          difficult.</t>

        <t>The Yeti KSK is rolled twice in Yeti testbed as of the
          writing.  In the first trial, it made old KSK inactive
          and new key active in one week after new key created, and
          deleted the old key in another week, which was totally
          unaware the timer specified in RFC5011. Because the
          hold-down timer was not correctly set in the server side,
          some clients (like Unbound) receive SERVFAILs (like dig
          without +cd) because the new key was still in AddPend
          state when old key was inactive. The lesson from the first
          KSK trial is that both server and client should compliant
          to RFC5011.</t>

        <t>For the second KSK rollover, it waited 30 days after a new
          KSK is published in the root zone. Different from ICANN
          rollover plan, it revokes the old key once the new key
          become active.  We don't want to wait too long, so we shorten
          the time for key publish and delete in server side. As of
          the writing, only one bug <xref target="KROLL-ISSUE"/>spotted
          on one Yeti resolver (using BIND 9.10.4-p2) during the
          second Yeti KSK rollover. The resolver is configured with
          multiple views before the KSK rollover. DNSSEC failures are
          reported once we added new view for new users after rolling
          the key. By checking the manual of BIND9.10.4-P2, it is
          said that unlike trusted-keys, managed-keys may only be set
          at the top level of named.conf, not within a view. It gives
          an assumption that for each view, managed-key can not be
          set per view in BIND. But right after setting the managed-keys
          of new views, the DNSSEC validation works for this view.
          As a conclusion for this issue, we suggest currently BIND
          multiple-view operation needs extra guidance for RFC5011.
          The manage-keys should be set carefully during the KSK
          rollover for each view when the it is created.</t>

        <t>Another of the questions of KSK rollover is how can an
          authority server know the resolver is ready for RFC5011.
          Two Internet-Drafts <xref target="I-D.wessels-edns-key-tag"/>
          and <xref target="I-D.wkumari-dnsop-trust-management"/> try
          to address the problem. In addition a compliant resolver
          implementation may fail without any complain if it is not
          correctly configured. In the case of Unbound 1.5.8, the key
          is only readable for DNS users <xref
          target="auto-trust-anchor-file"/>.</t>
      </section>

      <section title="Bigger ZSK for Yeti">
        <t>Currently IANA root system uses 1024-bits ZSK which is no
          longer recommended cryptography.  VeriSign announced at
          DNS-OARC 24th workshop that the IANA root zone ZSK will be
          increased from 1024 bits to 2048 bits in 2016. However, it
          is not fully tested by the real environment.</t>

        <t>Bigger key tend to produce a larger response which requires
          IP fragmentation and is commonly considered harm for DNS
          system. In Yeti DNS Project, it is desirable to test bigger
          responses in many aspects. The Big ZSK experiment is designed
          to test operating the Yeti root with a 2048-bit ZSK. The
          traffic is monitored before and after we lengthen the ZSK
          to see if there are any changes, such as a drop off of
          packets or a increase in retries. The current status of
          this experiment is under monitoring data analysis. </t>
      </section>
    </section>

    <section title="Other Technical findings and bugs">
      <t>Besides the experiments with specific goals and procedures,
        some unexpected bugs have been reported.  It is worthwhile
        to record them as technical findings from Yeti DNS Project.</t>

      <section title="IPv6 fragments issue">

        <t>Davey's note: Maybe we can move and extend it to section 4.3</t>

        <t>There are two cases in Yeti testbed reported that some
          Yeti root servers failed to pull the zone from a Distribution
          Master via AXFR/IXFR. Two facts have been revealed in
          both client side and server side after trouble shooting.</t>

        <t>One fact in client side is that some operation system
          can not handle IPv6 fragments correctly and AXRF/IXFR in
          TCP fails.  The bug covers several OSs and one VM platform
          (listed below).</t>

        <texttable>
          <ttcol>OS</ttcol>
          <ttcol>VM</ttcol>

          <c>NetBSD 6.1 and 7.0RC1</c><c>VMware ESXI 5.5</c>
          <c>FreeBSD10.0 </c><c></c>
          <c>Debian 3.2</c><c></c>
        </texttable>

        <t>Another fact is from server side in which one TCP segment
          of AXRF/ IXFR is fragmented in IP layer resulting in two
          fragmented packets.  This weird behavior has been documented
          IETF draft<xref target="I-D.andrews-tcp-and-ipv6-use-minmtu"/>.
          It reports a situation that some implementations of TCP
          running over IPv6 neglect to check the IPV6_USE_MIN_MTU
          value when performing MSS negotiation and when constructing
          a TCP segment.  It will cause TCP MSS option set to 1440
          bytes, but IP layer will limit the packet less than 1280
          bytes and fragment the packet to two fragmented packets.</t>

        <t>While the latter is not a technical error, but it will
          cause the error in the former fact which deserves much
          attention in IPv6 operation.</t>
      </section>

      <section title="Root name compression issue">
        <t><xref target="RFC1035"/>specifies DNS massage compression
          scheme which allows a domain name in a message to be
          represented as either: 1) a sequence of labels ending in
          a zero octet, 2) a pointer, 3) or a sequence of labels
          ending with a pointer.  It is designed to save more room
          of DNS packet.</t>

        <t>However in Yeti testbed, it is found that Knot 2.0 server
          compresses even the root.  It means in a DNS message the
          name of root (a zero octet) is replaced by a pointer of
          2 octets.  As well, it is legal but breaks some tools (Go
          DNS lib in this bug report) which does not expect such
          name compression for root.  Both Knot and Go DNS lib have
          fixed that bug by now.</t>
      </section>

      <section title= "SOA update delay issue">
        <t>It is observed one server on Yeti testbed have some
          bugs on SOA update with more than 10 hours delay. It
          is running on Bundy 1.2.0 on FreeBSD 10.2-RELEASE. A
          workaround is to check DM's SOA status in regular base.
          But it still need some work to find the bug in code
          path to improve the software.</t>
      </section>
    </section>

    <section title="IANA Considerations">
      <t>This document requests no action of the IANA.</t>
    </section>

    <section title="Acknowledgments">
      <t>The editors would like to acknowledge the contributions of
        the various and many subscribers to the Yeti DNS Project
        mailing lists, including the following people who were
        involved in the implementation and operation of the Yeti
        DNS testbed itself:</t>

      <t>
        <list style="empty">
	  <t>Tomohiro Ishihara, Antonio Prado, Stephane Bortzmeyer,
	    Mickael Jouanne, Pierre Beyssac, Joao Damas, Pavel
	    Khramtsov, Ma Yan, Otmar Lendl, Praveen Misra, Carsten
	    Strotmann, Edwin Gomez, Remi Gacogne, Guillaume de Lafond,
	    Yves Bovard, Hugo Salgado-Hernndez, Li Zhen, Daobiao
	    Gong, Runxia Wan.</t>
        </list>
      </t>

      <t>The editors also acknowledge the contributions of the
	Independent Submissions Editorial Board, and of the following
	reviewers whose opinions helped improve the clarity of this
	document:</t>

      <t>
        <list style="empty">
          <t>Subramanian Moonesamy, Joe Abley.</t>
        </list>
      </t>
    </section>
  </middle>

  <back>
    <references title="References">
      &RFC1034;
      &RFC1035;
      &RFC1996;
      &RFC2826;
      &RFC4968;
      &RFC7626;
      &RFC5011;
      &RFC5890;
      &RFC6891;
      &RFC7720;
      &I-D.muks-dns-message-fragments;
      &I-D.wkumari-dnsop-trust-management;
      &I-D.andrews-tcp-and-ipv6-use-minmtu;
      &I-D.wessels-edns-key-tag;
      &I-D.bortzmeyer-dname-root;
      &I-D.ietf-dnsop-resolver-priming;

      <reference anchor="ITI2014"
        target="https://www.icann.org/en/system/files/files/iti-report-15may14-en.pdf">
        <front>
          <title>Identifier Technology Innovation Report</title>
          <author/>
          <date day="15" month="May" year="2014"/>
        </front>
      </reference>

      <reference anchor="Fragmenting-IPv6"
        target="http://blog.apnic.net/2016/05/19/fragmenting-ipv6/">
        <front>
          <title>Fragmenting-IPv6</title>
          <author fullname="Geoff Huston" initials="G." surname="Huston"/>
          <date month="May" year="2016" />
        </front>
      </reference>

      <reference anchor="Renumbering-J-Root"
        target="https://indico.dns-oarc.net/event/24/session/10/contribution/10/material/slides/0.pdf">
        <front>
          <title>Thirteen Years of Old J-Root</title>
          <author fullname="Duane Wessels" initials="D." surname="Wessels"/>
          <date year="2015" />
        </front>
      </reference>

      <reference anchor="Root-Zone-Database"
        target="http://www.iana.org/domains/root/db">
        <front>
          <title>Root Zone Database</title>
          <author/>
          <date/>
        </front>
      </reference>

      <reference anchor="ICANN-ROOT-ROLL"
        target="https://www.iana.org/reports/2016/root-ksk-rollover-design-20160307.pdf">
        <front>
          <title>Root Zone KSK Rollover Plan</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="ROOT-FAQ"
        target="https://www.isoc.org/briefings/020/">
        <front>
          <title>DNS Root Name Server FAQ</title>
          <author fullname="Daniel Karrenberg" initials="D." surname="Karrenberg"/>
          <date year="2007" />
        </front>
      </reference>

      <reference anchor="pcapdump-bug-report"
        target="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=545985">
        <front>
          <title>pcaputils: IWBN to have an option to run a program
            after file rotation in pcapdump </title>
          <author fullname="Stephane Bortzmeyer"  initials="S."
            surname="Bortzmeyer"/>
          <date year="2009"/>
        </front>
      </reference>

      <reference anchor="Yeti-DNS-Project"
        target="http://www.yeti-dns.org">
        <front>
          <title>Website of Yeti DNS Project</title>
          <author/>
          <date />
        </front>
      </reference>

      <reference anchor="Experiment-MZSK-notes"
        target="https://github.com/shane-kerr/Yeti-Project/blob/experiment-mzsk/doc/Experiment-MZSK-notes.md">
        <front>
          <title>MZSK Experiment Notes</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="Yeti-DM-Sync-MZSK"
        target="https://github.com/BII-Lab/Yeti-Project/blob/master/doc/Yeti-DM-Sync-MZSK.md">
        <front>
          <title>Yeti DM Synchronization for MZSK</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="Hintfile-Auto-Update"
        target="https://github.com/BII-Lab/Hintfile-Auto-Update">
        <front>
          <title>Hintfile Auto Update</title>
          <author/>
          <date year="2015"/>
        </front>
      </reference>

      <reference anchor="auto-trust-anchor-file"
        target="https://www.nlnetlabs.nl/bugs-script/show_bug.cgi?id=758">
        <front>
          <title> Unbound should test that auto-* files are writable</title>
          <author/>
          <date year="2016"/>
        </front>
      </reference>

      <reference anchor="KROLL-ISSUE"
        target="http://yeti-dns.org/yeti/blog/2016/10/26/A-DNSSEC-issue-during-Yeti-KSK-rollover.html">
        <front>
          <title>A DNSSEC issue during Yeti KSK rollover</title>
          <author/>
          <date year="2016" />
        </front>
      </reference>

      <reference anchor="TNO2009"
        target="https://www.icann.org/en/system/files/files/root-scaling-model-description-29sep09-en.pdf">
        <front>
          <title>Root Scaling Study: Description of the DNS Root Scaling
            Model</title>
          <author fullname="Bart Gijsen" initials="B." surname="Gijsen"/>
          <author fullname="Almerima Jamakovic" initials="A."
            surname="Jamakovic"/>
          <author fullname="Frank Roijers" initials="F." surname="Roijers"/>
          <date day="29" month="September" year="2009"/>
        </front>
      </reference>

      <reference anchor="ISC-TN-2003-1"
        target="http://ftp.isc.org/isc/pubs/tn/isc-tn-2003-1.txt">
        <front>
          <title>Hierarchical Anycast for Global Service Distribution</title>
          <author fullname="Joe Abley" initials="J." surname="Abley"/>
          <date month="March" year="2003"/>
        </front>
      </reference>

      <reference anchor="RSSAC001"
        target="https://www.icann.org/en/system/files/files/rssac-001-root-service-expectations-04dec15-en.pdf">
        <front>
          <title>Service Expectations of Root Servers</title>
          <author/>
          <date day="4" month="December" year="2015"/>
        </front>
      </reference>

      <reference anchor="RSSAC023"
        target="https://www.icann.org/en/system/files/files/rssac-023-04nov16-en.pdf">
        <front>
          <title>History of the Root Server System</title>
          <author/>
          <date day="4" month="November" year="2016"/>
        </front>
      </reference>

      <reference anchor="RRL"
        target="http://www.redbarn.org/dns/ratelimits">
        <front>
          <title>Response Rate Limiting (RRL)</title>
          <author fullname="Paul Vixie" initials="P." surname="Vixie"/>
          <author fullname="Vernon Schryver" initials="V." surname="Schryver"/>
          <date day="10" month="June" year="2012"/>
        </front>
      </reference>
    </references>

    <section anchor="yeti-hints" title="Yeti-Root Hints File">
      <t>The following hints file (complete and accurate at the
	time of writing) causes a DNS resolver to use the Yeti DNS
	testbed in place of the production Root Server System and
	hence participate in experiments running on the testbed.</t>

      <t>Note that some lines have been wrapped in the text that
        follows in order to fit within the production constraints of
        this document. Wrapped lines are indicated with a blackslash
        character ("\"), following common convention.</t>

      <figure>
        <artwork>
          <![CDATA[
.                     3600000  IN   NS     bii.dns-lab.net
bii.dns-lab.net       3600000  IN   AAAA   240c:f:1:22::6
.                     3600000  IN   NS     yeti-ns.tisf.net
yeti-ns.tisf.net      3600000  IN   AAAA   2001:559:8000::6
.                     3600000  IN   NS     yeti-ns.wide.ad.jp
yeti-ns.wide.ad.jp    3600000  IN   AAAA   2001:200:1d9::35
.                     3600000  IN   NS     yeti-ns.as59715.net
yeti-ns.as59715.net   3600000  IN   AAAA   \
                           2a02:cdc5:9715:0:185:5:203:53
.                     3600000  IN   NS     dahu1.yeti.eu.org
dahu1.yeti.eu.org     3600000  IN   AAAA   \
                           2001:4b98:dc2:45:216:3eff:fe4b:8c5b
.                     3600000  IN   NS     ns-yeti.bondis.org
ns-yeti.bondis.org    3600000  IN   AAAA   2a02:2810:0:405::250
.                     3600000  IN   NS     yeti-ns.ix.ru
yeti-ns.ix.ru         3600000  IN   AAAA   2001:6d0:6d06::53
.                     3600000  IN   NS     yeti.bofh.priv.at
yeti.bofh.priv.at     3600000  IN   AAAA   2a01:4f8:161:6106:1::10
.                     3600000  IN   NS     yeti.ipv6.ernet.in
yeti.ipv6.ernet.in    3600000  IN   AAAA   2001:e30:1c1e:1::333
.                     3600000  IN   NS     yeti-dns01.dnsworkshop.org
yeti-dns01.dnsworkshop.org \
                      3600000  IN   AAAA   2001:1608:10:167:32e::53
.                     3600000  IN   NS     yeti-ns.conit.co
yeti-ns.conit.co      3600000  IN   AAAA   \
                          2604:6600:2000:11::4854:a010
.                     3600000  IN   NS     dahu2.yeti.eu.org
dahu2.yeti.eu.org     3600000  IN   AAAA   2001:67c:217c:6::2
.                     3600000  IN   NS     yeti.aquaray.com
yeti.aquaray.com      3600000  IN   AAAA   2a02:ec0:200::1
.                     3600000  IN   NS     yeti-ns.switch.ch
yeti-ns.switch.ch     3600000  IN   AAAA   2001:620:0:ff::29
.                     3600000  IN   NS     yeti-ns.lab.nic.cl
yeti-ns.lab.nic.cl    3600000  IN   AAAA   2001:1398:1:21::8001
.                     3600000  IN   NS     yeti-ns1.dns-lab.net
yeti-ns1.dns-lab.net  3600000  IN   AAAA   2001:da8:a3:a027::6
.                     3600000  IN   NS     yeti-ns2.dns-lab.net
yeti-ns2.dns-lab.net  3600000  IN   AAAA   2001:da8:268:4200::6
.                     3600000  IN   NS     yeti-ns3.dns-lab.net
yeti-ns3.dns-lab.net  3600000  IN   AAAA   2400:a980:30ff::6
.                     3600000  IN   NS     \
                        ca978112ca1bbdcafac231b39a23dc.yeti-dns.net
ca978112ca1bbdcafac231b39a23dc.yeti-dns.net \
                      3600000  IN   AAAA   2c0f:f530::6
.                     3600000  IN   NS     \
                        3e23e8160039594a33894f6564e1b1.yeti-dns.net
3e23e8160039594a33894f6564e1b1.yeti-dns.net \
                      3600000  IN   AAAA   2803:80:1004:63::1
.                     3600000  IN   NS     \
                        3f79bb7b435b05321651daefd374cd.yeti-dns.net
3f79bb7b435b05321651daefd374cd.yeti-dns.net \
                      3600000  IN   AAAA   2401:c900:1401:3b:c::6
.                     3600000  IN   NS     \
                        xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c
xn--r2bi1c.xn--h2bv6c0a.xn--h2brj9c \
                      3600000  IN   AAAA   2001:e30:1c1e:10::333
.                     3600000  IN   NS     yeti1.ipv6.ernet.in
yeti1.ipv6.ernet.in   3600000  IN   AAAA   2001:e30:187d::333
.                     3600000  IN   NS     yeti-dns02.dnsworkshop.org
yeti-dns02.dnsworkshop.org \
                      3600000  IN   AAAA   2001:19f0:0:1133::53
.                     3600000  IN   NS     yeti.mind-dns.nl
yeti.mind-dns.nl      3600000  IN   AAAA   2a02:990:100:b01::53:0
]]>
        </artwork>
      </figure>
    </section>

    <section title="About This Document">
      <t>This section (and sub-sections) has been included as an
        aid to reviewers of this document, and should be removed
        prior to publication.</t>

      <section title="Venue">
        <t>The authors propose that this document proceeed as an
          Independent Submission, since it documents work that,
          although relevant to the IETF, has been carried out
          externally to any IETF working group. However, a suitable
          venue for discussion of this document is the dnsop working
          group.</t>

        <t>Information about the Yeti DNS project and discussion
          relating to particular experiments described in this
          document can be found at <eref
          target="https://yeti-dns.org/"/>.</t>

        <t>This document is maintained in GitHub at
          <eref target="https://github.com/BII-Lab/yeti-testbed-experience"/>.</t>
      </section>

      <section title="Revision History">
        <section title="draft-song-yeti-testbed-experience-04">
          <t>Add Joe Abley as co-editor. Substantial editorial review.</t>
        </section>
      </section>
    </section>
  </back>
</rfc>
